apiVersion: v1
kind: ConfigMap
metadata:
  name: open-science-ce-{{ .Values.Instance }}-osg-configuration
  labels:
    app: open-science-ce
    chart: {{ template "open-science-ce.chart" . }}
    instance: {{ .Values.Instance }}
    release: {{ .Release.Name }}
data:
  99-local.ini: |
    [Site Information]
    {{ if .Values.Developer.Enabled }}
    group = OSG-ITB
    {{ else }}
    group = OSG
    {{ end }}
    host_name = localhost
    resource = {{ .Values.Site.Resource }}
    resource_group = {{ .Values.Site.ResourceGroup }}
    sponsor = {{ .Values.Site.Sponsor }}
    site_policy = UNAVAILABLE
    contact = {{ .Values.Site.Contact }}
    email = {{ .Values.Site.ContactEmail }}
    city = {{ .Values.Site.City }}
    country = {{ .Values.Site.Country }}
    latitude = {{ .Values.Site.Latitude }}
    longitude = {{ .Values.Site.Longitude }}

    [BOSCO]
    enabled = False

    [Misc Services]
    edit_lcmaps_db = True
    authorization_method = vomsmap

    [Storage]
    se_available = False
    default_se = None
    grid_dir = {{ .Values.Storage.GridDir }}
    app_dir = UNSET
    data_dir = None
    worker_node_temp = {{ .Values.Storage.WorkerNodeTemp }}

    [Subcluster {{ .Values.Site.Resource }}]
    name = {{ .Values.Site.Resource }}
    ram_mb = {{ .Values.Cluster.Memory }}
    cores_per_node = {{ .Values.Cluster.CoresPerNode }}
    max_wall_time = {{ .Values.Cluster.MaxWallTime }}
    allowed_vos = {{ .Values.Cluster.AllowedVOs }}

    [Squid]
    {{ if .Values.Squid.Location }}
    enabled = True
    {{ else }}
    enabled = False
    {{ end }}
    location =  {{ .Values.Squid.Location }}
    policy = DEFAULT
    cache_size = DEFAULT
    memory_size = DEFAULT
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: open-science-ce-{{ .Values.Instance }}-htcondor-ce-configuration
  labels:
    app: open-science-ce
    chart: {{ template "open-science-ce.chart" . }}
    instance: {{ .Values.Instance }}
    release: {{ .Release.Name }}
data:
  99-instance.conf: |+
    # osg-flock packaging now uses IDTOKENS and we rely on flock as
    # the submit host collector
    JOB_ROUTER_SCHEDD_NAME = $(NETWORK_HOSTNAME)
    JOB_ROUTER_SCHEDD2_POOL = flock.opensciencegrid.org:9618
    # Force container daemons to talk over the container IP
    PRIVATE_NETWORK_NAME = $(UTSNAME_NODENAME)
    PRIVATE_NETWORK_INTERFACE = $(IP_ADDRESS)

    # Special magic so that jobs routed to the VO pool are accounted for as payload jobs
    JOB_ROUTER_DEFAULTS @=jrd
    $(JOB_ROUTER_DEFAULTS)
    [
      set_JOBGLIDEIN_ResourceName = "$$([IfThenElse(IsUndefined(TARGET.GLIDEIN_ResourceName), IfThenElse(IsUndefined(TARGET.GLIDEIN_Site), \"Local Job\", TARGET.GLIDEIN_Site), TARGET.GLIDEIN_ResourceName)])";
    ]
    @jrd
{{ if .Values.Networking.Hostname }}
    # Force HTCondor to use the requested hostname
    NETWORK_HOSTNAME = {{ .Values.Networking.Hostname }}
    # For a given CE, use the same token trust domain between instances
    TRUST_DOMAIN = {{ .Values.Networking.Hostname }}
{{ end }}
{{ if .Values.Networking.RequestIP }}
    TCP_FORWARDING_HOST = {{ .Values.Networking.RequestIP }}
{{ end }}
{{ .Values.HTCondorCeConfig | default "" | indent 4 }}
{{ if .Values.VomsmapOverride }}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: open-science-ce-{{ .Values.Instance }}-voms-mapfile
  labels:
    app: open-science-ce
    chart: {{ template "open-science-ce.chart" . }}
    instance: {{ .Values.Instance }}
    release: {{ .Release.Name }}
data:
  voms-mapfile: |+
{{ .Values.VomsmapOverride | indent 4 }}
{{ end }}
{{ if .Values.GridmapOverride }}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: open-science-ce-{{ .Values.Instance }}-grid-mapfile
  labels:
    app: open-science-ce
    chart: {{ template "open-science-ce.chart" . }}
    instance: {{ .Values.Instance }}
    release: {{ .Release.Name }}
data:
  grid-mapfile: |+
{{ .Values.GridmapOverride | indent 4 }}
{{ end }}
{{ if .Values.HTTPLogger.Enabled }}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: open-science-ce-{{ .Values.Instance }}-logger-startup
  labels:
    app: open-science-ce
    chart: {{ template "open-science-ce.chart" . }}
    instance: {{ .Values.Instance }}
    release: {{ .Release.Name }}
data:
  start-nginx.sh: |+
    #!/bin/bash -e
    
    apt-get update
    apt-get install openssl -y
  
    if [ -z $HTPASSWD ]; then
      PASS=$(tr -dc 'a-f0-9' < /dev/urandom | head -c16)
      echo "Your randomly generated logger credentials are"
      echo "**********************************************"
      echo "logger:$PASS"
      echo "**********************************************"
      HTPASSWD="$(openssl passwd -apr1 $(echo -n $PASS))"
    fi
  
    # maybe validate this
    mkdir -p /etc/nginx/auth
    echo "logger:$HTPASSWD" > /etc/nginx/auth/htpasswd

    echo 'server {
      listen       8080;
      server_name  localhost;
      location / {
        default_type text/plain;
        auth_basic "Restricted";
        auth_basic_user_file /etc/nginx/auth/htpasswd;  
        root   /usr/share/nginx/html;
        autoindex  on;
      }
      error_page   500 502 503 504  /50x.html;
      location = /50x.html {
        root   /usr/share/nginx/html;
      }
    }' > /etc/nginx/conf.d/default.conf
    exec nginx -g 'daemon off;'

{{ end }}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: open-science-ce-{{ .Values.Instance }}-condor-configuration
  labels:
    app: open-science-ce
    chart: {{ template "open-science-ce.chart" . }}
    instance: {{ .Values.Instance }}
    release: {{ .Release.Name }}
data:
  condor_config.local: |+
    # Allow the CE to route jobs as the mapped user
    QUEUE_SUPER_USER_MAY_IMPERSONATE = .*
    # Force container daemons to talk over the container IP
    PRIVATE_NETWORK_NAME = $(UTSNAME_NODENAME)
    PRIVATE_NETWORK_INTERFACE = $(IP_ADDRESS)

{{ if .Values.Networking.Hostname }}
    # Force HTCondor to use the requested hostname
    NETWORK_HOSTNAME = {{ .Values.Networking.Hostname }}
    # For a given CE, use the same token trust domain between instances
    TRUST_DOMAIN = {{ .Values.Networking.Hostname }}
{{ end }}
{{ if .Values.Networking.RequestIP }}
    TCP_FORWARDING_HOST = {{ .Values.Networking.RequestIP }}
{{ end }}
{{ if .Values.Submit.ConfigFile }}
{{ .Values.Submit.ConfigFile  | indent 4 }}
{{ end }}

  ProbeConfig: |+
    <ProbeConfiguration

        Title1="Collector Information"

        CollectorHost="gratia-osg-prod.opensciencegrid.org:80"
        SSLHost="gratia-osg-prod.opensciencegrid.org:443"
        SSLRegistrationHost="gratia-osg-prod.opensciencegrid.org:80"

        CollectorService="/gratia-servlets/rmi"
        SSLCollectorService="/gratia-servlets/rmi"
        RegistrationService="/gratia-registration/register"

        Title2="Probe information and functional configuration"

        ProbeName="condor:{{ .Values.Networking.Hostname }}"
        SiteName="{{ .Values.Site.Resource }}"
        Grid="OSG"
        SuppressUnknownVORecords="0"
        SuppressNoDNRecords="0"
        SuppressGridLocalRecords="1"
        QuarantineUnknownVORecords="1"
        EnableProbe="1"

        MapUnknownToGroup="1"
        MapGroupToRole="1"
        VOOverride="osg"

        Title3="Tuning parameter"

        BundleSize="100"
        MaxPendingFiles="100000"
        MaxStagedArchives="400"
        UseSyslog="0"
        ConnectionTimeout="900"

        LogLevel="2"
           Comments32="Controls debug messages printed to log file."
        DebugLevel="0"
           Comments33="Controls debug messages printed to screen."
        LogRotate="31"
        LogFileName=""
           Comments34="If LogFileName is set, then there is no log file rotation. Otherwise Log file rotation will create one new file each day and delete the older ones"
        DataLengthMax="0"
           Comments35="The maximum number of records (or hours, depending on the probe) processed in a single invocation, 0 = all records, no max"
        DataFileExpiration="31"
           Comments36="The number of days quarantined and unusable data files are kept"
        QuarantineSize="200"
           Comments37="The maximum size in Mb allowed to be kept in each quarantined directory"
        GratiaExtension="gratia.xml"

        Title4="Authentication Configuration"

        UseSSL="0"
        CertificateFile="/etc/grid-security/hostcert.pem"
        KeyFile="/etc/grid-security/hostkey.pem"
        UseGratiaCertificates="0"
           Comments40="If no directory is specified the gratia certificate file will be created in 'WorkingFolder'/certs."
        GratiaCertificateFile="/var/lib/gratia/data/certs/gratia.probecert.pem"
        GratiaKeyFile="/var/lib/gratia/data/certs/gratia.probekey.pem"

        Title5="File and directory location"

        UserVOMapFile="/var/lib/osg/user-vo-map"
            Comments51="Location and wildcard pattern of log files that contains certificate information about the jobs in the format followed by the 'blah demons'."
        CertInfoLogPattern="/var/log/accounting/blahp.log-*"
        CondorCEHistoryFolder="/var/lib/gratia/condorce_data"

        DataFolder="/var/lib/gratia/data/"
        WorkingFolder="/var/lib/gratia/tmp"
        LogFolder="/var/log/gratia"


        CondorScheddName=""
          Comments91="Use this to set the name of the scheduler, if there are mutliple."
        NoCertinfoBatchRecordsAreLocal="0"

    />
  # end ProbeConfig
